---
layout: article
title: DQN 논문 정리
tags: Paper_Review
#comments: true
#article_header:
#  type: cover
#  image:
#    src:
aside:
  toc: true
key: page-aside
use_math: true
---

  \* 이 포스트는 DeepMind의  
  [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)    
  [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)  
  논문에 대한 내용을 정리한 글입니다.

  ----------------------------------------------------------------------

**분류** : Model-Free/Deep Q-Learning/DQN  


## Domain Knowledge

  &nbsp;&nbsp;DQN 논문이 발표되기 전까지 vision이나 speech같은 high-dimensional sensory data로부터 agent를 학습시키는 것은 RL의 오랜 과제였습니다. RL agent가 다양한 domain에서 성공적인 결과를 내놓긴 했지만, 여전히 그 실용성은 hand-crafted feature를 얻을 수 있는 domain이나 low-dimension의 state를 가지는 domain에 한정되어 있었습니다. 2000년대 이후로 Deep Learning이 발전하면서 Neural Network를 사용해 고차원의 feature를 얻는 것이 가능해졌고 이를 RL에도 적용하려는 연구가 진행되었는데, 몇 가지 이유들로 학습이 불안정하고 diverge합니다.

  1. RL에서는 학습 data로 reward만 사용할 수 있고 이 또한 sparse, noisy, delayed 하기 때문에 학습에 어려움이 있습니다.

  2. RL은 sequentail decision 문제를 다루기 때문에 sequential observation에는 data correlation이 존재합니다.

  3. 알고리즘이 새로운 behavior(e.g. policy)를 학습하는 과정에서 새롭게 얻게 되는 data의 distribution이 변하게 되므로 학습이 불안정합니다.

  이러한 문제들을 2013 NIPS에 발표한 [DQN 논문](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)에서 **Experience Replay**로 해결했고, 이후 2015 Nature에 발표한 [DQN 논문](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)에서 Q-learning의 target이 계속 변하는 Non-stationary 문제를 **Target Network**로 해결했습니다.

## Problems

  DQN 모델은 Model-free, off-policy 알고리즘인 Q-learning을 기반으로 학습합니다. Agent는 환경과의 상호작용으로 발생하는 episode들을 sampling하면서 환경의 dynamics에 대한 정보를 얻고 optimal action-value(e.g. Q-function)을 구하는 것으로 optimal policy를 학습할 수 있습니다. optimal Q-function은 수식으로 표현하면 다음과 같이 나타낼 수 있습니다.  

$$
\begin{aligned}
Q^*(s, a) = \mathbb{E}_{s' \sim \epsilon} [ r + \gamma max_{a'} Q^*(s', a') \mid s, \; a ]\\
\end{aligned}
$$

  여기서 Q-function을 다음과 같이 Neural network로 근사할 수 있습니다.  

$$
\begin{aligned}
Q(s, a; \theta) \simeq Q^*(s, a)\\
\end{aligned}
$$

  그러나 RL에서는 Neural network와 같은 Non-linear function approximator를 사용하면 제대로 학습이 되지 않는 것으로 알려져 있습니다.  

<p align="center"><img src="https://github.com/LoteeYoon/LoteeYoon.github.io/blob/master/Convergence.JPG?raw=true"></p>

  이는 [Domain Knowledge](#domain-knowledge)에서 설명한 문제들 때문인데, 좀 더 자세히 살펴보겠습니다.  
<br/>

### 1. Sample Correlation

  Simulation을 통해 얻은 trajectory에서 인접한 sample data의 경우에는 상당히 유사한 정보를 담고 있을 확률이 높기 때문에 correlation이 높을 것입니다. 만약 correlation이 높은 data들이 sampling 된다면, 아래의 그림과 같이 근사하는 function 역시 특정 sample들에만 최적화될 것이므로 학습이 제대로 되지 않습니다. 따라서 학습을 잘 시키기 위해선 correlation이 낮은 sample을 모으는 것이 관건입니다.


<p align="center"><img src="https://github.com/LoteeYoon/LoteeYoon.github.io/blob/master/correlation.JPG?raw=true"></p>  


### 2. Non-stationary targets

  위와 같이 Q-function을 function approximator $\theta$ 를 통해 Neural network로 근사하고 나면 이를 학습하기 위한 Loss function이 필요합니다. Loss function은 다음과 같이 Squared Error 형태로 정의할 수 있습니다.  

$$
\begin{aligned}
L_{i}(\theta_{i}) &= \mathbb{E}_{s, a \sim \rho(\cdot)} [ (y_i - Q(s, a; \theta_{i}))^2 ]\\
where, \; y_i &= \mathbb{E}_{s' \sim \epsilon} [ r + \gamma \max_{a'} Q(s', a'; \theta_{i-1}) \mid s, \; a]\\
\end{aligned}
$$

  하지만 여기서 Prediction $Q(s, a; \; \theta_{i})$와 target $y_i$에 존재하는 $Q(s', a'; \; \theta_{i-1})$는 동일한 network parameter $\theta$ 에 의해 계산되기 때문에 target과 prediction이 계속 변화하여 학습이 unstable하다는 문제가 있습니다.


## Methods
