---
layout: article
title: DQN 논문 정리
tags: Paper_Review
#comments: true
#article_header:
#  type: cover
#  image:
#    src:
aside:
  toc: true
key: page-aside
use_math: true
---

  \* 이 포스트는 DeepMind의  
  [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)    
  [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)  
  논문에 대한 내용을 정리한 글입니다.

  ----------------------------------------------------------------------

**분류** : Model-Free/Deep Q-Learning/DQN  


## Domain Knowledge

  &nbsp;&nbsp;DQN 논문이 발표되기 전까지 vision이나 speech같은 high-dimensional sensory data로부터 agent를 학습시키는 것은 RL의 오랜 과제였습니다. RL agent가 다양한 domain에서 성공적인 결과를 내놓긴 했지만, 여전히 그 실용성은 hand-crafted feature를 얻을 수 있는 domain이나 low-dimension의 state를 가지는 domain에 한정되어 있었습니다. 2000년대 이후로 Deep Learning이 발전하면서 Neural Network를 사용해 고차원의 feature를 얻는 것이 가능해졌고 이를 RL에도 적용하려는 연구가 진행되었는데, 몇 가지 이유들로 학습이 불안정하고 diverge합니다.

  1. RL에서는 학습 data로 reward만 사용할 수 있고 이 또한 sparse, noisy, delayed 하기 때문에 학습에 어려움이 있습니다.

  2. RL은 sequentail decision 문제를 다루기 때문에 sequential observation에는 data correlation이 존재합니다.

  3. 알고리즘이 새로운 behavior(e.g. policy)를 학습하는 과정에서 새롭게 얻게 되는 data의 distribution이 변하게 되므로 학습이 불안정합니다.

  이러한 문제들을 2013 NIPS에 발표한 [DQN 논문](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)에서 **Experience Replay**로 해결했고, 이후 2015 Nature에 발표한 [DQN 논문](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)에서 Q-learning의 target이 계속 변하는 Non-stationary 문제를 **Target Network**로 해결했습니다.

## Problems

  DQN 모델은 Model-free, off-policy 알고리즘인 Q-learning을 기반으로 학습합니다. Agent는 환경과의 상호작용으로 발생하는 episode들을 sampling하면서 환경의 dynamics에 대한 정보를 얻고 optimal action-value(e.g. Q-function)을 구하는 것으로 optimal policy를 학습할 수 있습니다. optimal Q-function은 수식으로 표현하면 다음과 같이 나타낼 수 있습니다.  

$$
\begin{aligned}
Q^*(s, a) = \mathbb{E}_{s' \sim \epsilon} [ r + \gamma max_{a'} Q^*(s', a') \mid s, \; a ]\\
\end{aligned}
$$

  여기서 Q-function을 다음과 같이 Neural network로 근사할 수 있습니다.  

$$
\begin{aligned}
Q(s, a; \theta) \simeq Q^*(s, a)\\
\end{aligned}
$$

  그러나 RL에서는 Neural network와 같은 Non-linear function approximator를 사용하면 제대로 학습이 되지 않는 것으로 알려져 있습니다.  

<p align="center"><img src="https://github.com/LoteeYoon/LoteeYoon.github.io/blob/master/Convergence.JPG?raw=true"></p>

  이는 [Domain Knowledge](#domain-knowledge)에서 설명한 문제들 때문인데, 좀 더 자세히 살펴보겠습니다.  
<br/>

### 1. Sample Correlation

  Simulation을 통해 얻은 trajectory에서 인접한 sample data의 경우에는 상당히 유사한 정보를 담고 있을 확률이 높기 때문에 correlation이 높을 것입니다. 만약 correlation이 높은 data들이 sampling 된다면, 아래의 그림과 같이 근사하는 function 역시 특정 sample들에만 최적화될 것이므로 학습이 제대로 되지 않습니다. 따라서 학습을 잘 시키기 위해선 correlation이 낮은 sample을 모으는 것이 관건입니다.


<p align="center"><img src="https://github.com/LoteeYoon/LoteeYoon.github.io/blob/master/correlation.JPG?raw=true"></p>  


### 2. Non-stationary targets

  위와 같이 Q-function을 function approximator $\theta$ 를 통해 Neural network로 근사하고 나면 이를 학습하기 위한 Loss function이 필요합니다. Loss function은 다음과 같이 Squared Error 형태로 정의할 수 있습니다.  

$$
\begin{aligned}
L_{i}(\theta_{i}) &= \mathbb{E}_{s, a \sim \rho(\cdot)} [ (y_i - Q(s, a; \theta_{i}))^2 ]\\
where, \; y_i &= \mathbb{E}_{s' \sim \epsilon} [ r + \gamma \max_{a'} Q(s', a'; \theta_{i-1}) \mid s, \; a]\\
\end{aligned}
$$

  하지만 여기서 Prediction $Q(s, a; \; \theta_{i})$와 target $y_i$에 존재하는 $Q(s', a'; \; \theta_{i-1})$는 동일한 network parameter $\theta$ 에 의해 계산되기 때문에 target과 prediction이 계속 변화하여 학습이 unstable하다는 문제가 있습니다. 이는 마치 가려진 과녁을 화살로 맞추기 위해 움직였는데 또 다시 과녁이 움직여 가려지는 상황이 반복되는 것과 같습니다. 이 같은 문제를 해결하기 위해서는 고정된 과녁이 필요합니다.


## Methods

### 1. Experience Replay

  논문에서는 sample data간의 correlation을 없애기 위해 여러 trajectory에서 transition sample들을 모아 **Replay Buffer**에 저장하고 이를 sampling하여 학습에 사용하는 방법을 제안합니다. Q-learning은 off-policy이므로 다른 policy에서 생성된 data로도 학습이 가능하다는 장점이 있기 때문에 과거의 policy로부터 생성된 data를 사용해 학습할 수 있습니다.

  <p align="center"><img src="https://github.com/LoteeYoon/LoteeYoon.github.io/blob/master/replaybuffer.JPG?raw=true"></p>  

  **Replay Buffer**는 sample들 사이 correlation 문제를 해결하기도 하지만 한 번 사용하고 버려지는 sample들을 저장했다가 다시 사용할 수 있으므로 sample efficient하다는 이점도 있습니다.
<br/>

### 2. Target Network

  우리는 앞에서 Loss function을 다음과 같이 나타낼 수 있었습니다.

$$
\begin{aligned}
L_{i}(\theta_{i}) &= \mathbb{E}_{s, a \sim \rho(\cdot)} [ (y_i - Q(s, a; \theta_{i}))^2 ]\\
where, \; y_i &= \mathbb{E}_{s' \sim \epsilon} [ r + \gamma \max_{a'} Q(s', a'; \textcolor{red}{\theta_{i-1}}) \mid s, \; a]\\
\end{aligned}
$$

  이때, target의 Q-function은 prediction의 Q-function과 동일한 network parameter에 의해 update되기 때문에 학습을 진행하면서 변화하는 moving target문제가 발생합니다. 이를 해결하기 위해 논문에서는 별도의 target network를 하나 더 만들어 일정한 주기로 update해주는 방법을 제안합니다. 이렇게 하면 target의 Q-function을 별도의 parameter $\theta^-$로 주기적으로 update하기 때문에 학습 도중 prediction을 계산하는 과정에서 target이 변하는 문제는 발생하지 않게 됩니다. 이를 다시 적용해 Loss function을 정리하면 다음과 같이 나타낼 수 있습니다.

$$
\begin{aligned}
L_{i}(\theta_{i}) &= \mathbb{E}_{s, a \sim \rho(\cdot)} [ (y_i - Q(s, a; \theta_{i}))^2 ]\\
where, \; y_i &= \mathbb{E}_{s' \sim \epsilon} [ r + \gamma \max_{a'} Q(s', a'; \theta^-_{i-1}) \mid s, \; a]\\
\end{aligned}
$$

  <br/>



  <p align="center"><img src="https://github.com/LoteeYoon/LoteeYoon.github.io/blob/master/dqn_architecture.jpg?raw=true"></p>  
