---
layout: article
title: DQN 논문 정리
tags: Paper_Review
#comments: true
#article_header:
#  type: cover
#  image:
#    src:
aside:
  toc: true
key: page-aside
---

  \* 이 포스트는 DeepMind의 [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) 논문에 대한 내용을 정리한 글입니다.

  ----------------------------------------------------------------------

**분류** : Model-Free/Deep Q-Learning/DQN  


## Abstract

  - high-dimensional sensory input이 주어졌을 때, agent를 효과적으로 학습시키는 deep learning 알고리즘을 제안  
  - CNN model을 사용해 raw pixel을 input으로 받으면 future reward를 추정하는 value function을 output  
  - Atari 2600의 6종 아케이드 게임에서 기존 연구를 능가했으며, 그 중 3종의 게임에 대해서 human expert를 능가  


## Introduction

  이 논문이 발표되기 전까지 vision이나 speech같은 high-dimensional sensory data로부터 agent를 학습시키는 것은 RL의 오랜 과제였습니다. 최근의 Deep Learning의 발전으로 고차원의 feature를 얻는 것이 가능해지면서 이를 RL에도 적용하려는 연구가 진행되었습니다. 그러나 Deep Learning을 RL에 적용하는 과정에서 몇 가지 문제를 발견하게 됩니다.  

    1) Deep Learning의 학습에는 labeling된 대량의 데이터가 필요한데, RL의 data에는 reward밖에 주어지지 않으며 이 또한 sparse, noisy, delayed 합니다. 
