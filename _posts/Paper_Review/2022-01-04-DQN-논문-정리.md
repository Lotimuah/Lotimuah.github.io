---
layout: article
title: DQN 논문 정리
tags: Paper_Review
#comments: true
#article_header:
#  type: cover
#  image:
#    src:
aside:
  toc: true
key: page-aside
use_math: true
---

  \* 이 포스트는 DeepMind의  
  [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)    
  [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)  
  논문에 대한 내용을 정리한 글입니다.

  ----------------------------------------------------------------------

**분류** : Model-Free/Deep Q-Learning/DQN  


## Problem

  &nbsp;&nbsp;이 논문이 발표되기 전까지 vision이나 speech같은 high-dimensional sensory data로부터 agent를 학습시키는 것은 RL의 오랜 과제였다. 최근의 Deep Learning의 발전으로 고차원의 feature를 얻는 것이 가능해지면서 이를 RL에도 적용하려는 연구가 진행되었는데, Deep Learning을 RL에 적용하는 과정에는 몇 가지 문제가 존재한다.

  &nbsp;&nbsp;&nbsp;1. Deep Learning의 학습에는 labeling된 대량의 데이터가 필요한지만, RL에서는 label이 없는 reward로 학습하며 심지어 sparse, noisy, delayed 하므로 학습에 어려움이 있다.  

  &nbsp;&nbsp;&nbsp;2. 대부분의 Deep Learning 문제에서는 data의 independent를 가정하지만, sequential decision 문제를 다루는 RL의 특성상 data간의 correlation이 존재하기 때문에 가정이 성립하지 않는다. (Correlation Problem)  

  &nbsp;&nbsp;&nbsp;3. RL에서는 알고리즘이 새로운 behavior를 학습할 때마다 data의 distribution이 변하게 되므로 학습이 불안정하다. (Non-Stationary Problem)  

  &nbsp;&nbsp;본 논문에서는 기존의 Q-learning에서 Q-table을 **network**로 대체하여 raw image data로부터 control policy를 학습한다. 또한 **Experience Replay**와 **Target network**같은 아이디어들을 사용해 deep RL의 문제들을 해결한다.


## Methods

  Q-learning은 Model-free RL이므로 Agent는 환경과의 상호작용으로 발생하는 일련의 Observation(s), Action(a), Reward(r)를 통해 학습한다. Q-learning은 off-policy TD 알고리즘이고 이에 대한 Optimal action-value Q를 나타내면 다음과 같다.  

$$
\begin{aligned}
Q^*(s, a) = \mathbb{E}_{s' \sim \epsilon} [ r + \gamma max_{a'} Q^*(s', a') \mid s, \; a ]\\
\end{aligned}
$$

  이때 network를 function approximator로 Optimal action-value function Q를 근사할 수 있다.  

$$
\begin{aligned}
Q(s, a; \theta) \simeq Q^*(s, a)\\
\end{aligned}
$$

  Q-function을 parameter $\theta$ 로 근사한 network를 Q-network라고 한다. 그리고 Loss function을 다음과 같이 Squared Error 형태로 정의할 수 있다.

$$
\begin{aligned}
L_{i}(\theta_{i}) &= \mathbb{E}_{s, a \sim \rho(\cdot)} [ (y_i - Q(s, a; \theta_{i}))^2 ]\\
where, \; y_i &= \mathbb{E}_{s' \sim \epsilon} [ r + \gamma \max_{a'} Q(s', a'; \theta_{i-1}) \mid s, \; a]\\
\end{aligned}
$$


  하지만 RL에서는 Q-function을 나타내기 위해 Neural network같은 Non-linear function approximator를 사용하면 학습이 제대로 되지 않는 것으로 알려져 있다.


  <p align="center">
    <img src="https://raw.github.com/LoteeYoon/LoteeYoon.github.io/master/Convergence_of_Prediction_Algorithms.PNG">
  </p>
  <!-- ![png](/Convergence of Prediction Algorithms.PNG) -->

  이는 [Problem](#problem)에서 설명한 문제들 때문이다.

  - Experience replay

    이 method는 입력 데이터 사이의 correlation을 줄이기 위해 사용되는 방법이다.

  - Target Network


















<!-- - high-dimensional sensory input이 주어졌을 때, agent를 효과적으로 학습시키는 deep learning 알고리즘을 제안  
- CNN model을 사용해 raw pixel을 input으로 받으면 future reward를 추정하는 value function을 output  
- Atari 2600의 6종 아케이드 게임에서 기존 연구를 능가했으며, 그 중 3종의 게임에 대해서 human expert를 능가 -->
