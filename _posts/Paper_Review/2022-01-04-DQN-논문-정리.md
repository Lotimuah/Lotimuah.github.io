---
layout: article
title: DQN 논문 정리
tags: Paper_Review
#comments: true
#article_header:
#  type: cover
#  image:
#    src:
aside:
  toc: true
key: page-aside
use_math: true
---

  \* 이 포스트는 DeepMind의  
  [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)    
  [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)  
  논문에 대한 내용을 정리한 글입니다.

  ----------------------------------------------------------------------

**분류** : Model-Free/Deep Q-Learning/DQN  


## Domain Knowledge

  &nbsp;&nbsp;DQN 논문이 발표되기 전까지 vision이나 speech같은 high-dimensional sensory data로부터 agent를 학습시키는 것은 RL의 오랜 과제였습니다. RL agent가 다양한 domain에서 성공적인 결과를 내놓긴 했지만, 여전히 그 실용성은 hand-crafted feature를 얻을 수 있는 domain이나 low-dimension의 state를 가지는 domain에 한정되어 있었습니다. 2000년대 이후로 Deep Learning이 발전하면서 Neural Network를 사용해 고차원의 feature를 얻는 것이 가능해졌고 이를 RL에도 적용하려는 연구가 진행되었는데, 몇 가지 이유들로 학습이 불안정하고 diverge합니다.

  1. RL에서는 학습 data로 reward만 사용할 수 있고 이 또한 sparse, noisy, delayed 하기 때문에 학습에 어려움이 있습니다.

  2. RL은 sequentail decision 문제를 다루기 때문에 sequential observation에는 data correlation이 존재합니다.

  3. 알고리즘이 새로운 behavior(e.g. policy)를 학습하는 과정에서 새롭게 얻게 되는 data의 distribution이 변하게 되므로 학습이 불안정합니다.

  이러한 문제들을 2013 NIPS에 발표한 [DQN 논문](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)에서 **Experience Replay**로 해결했고, 이후 2015 Nature에 발표한 [DQN 논문](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)에서 Q-learning의 target이 계속 변하는 Non-stationary 문제를 **Target Network**로 해결했습니다.

## Problems

### 1. Sample Correlation

  DQN 모델은 Model-free, off-policy 알고리즘인 Q-learning을 기반으로 학습합니다. Agent는 환경과의 상호작용으로 발생하는 episode들을 sampling하면서 환경의 dynamics에 대한 정보를 얻고 optimal action-value(e.g. Q-function)을 구하는 것으로 optimal policy를 학습할 수 있습니다. 여기서 Q-function을 Neural network로 근사하는데, RL에서는 Neural network와 같은 Non-linear function approximator를 사용하면 제대로 학습이 되지 않는 것으로 알려져 있습니다.  

<p align="center"><img src="https://github.com/LoteeYoon/LoteeYoon.github.io/blob/master/Convergence.JPG?raw=true"></p>

  이는 [Problem](#problem)에서 설명한 문제들 때문인데, 좀 더 자세히 살펴보겠습니다.  
<br/>

  Simulation을 통해 얻은 trajectory에서 인접한 sample data의 경우에는 상당히 유사한 정보를 담고 있을 확률이 높기 때문에 correlation이 높을 것입니다. 만약 correlation이 높은 data들이 sampling 된다면 아래의 그림과 같이 근사하는 function 역시 특정 sample들에만 최적화될 것이므로 학습이 제대로 되지 않습니다.


<p align="center"><img src="https://github.com/LoteeYoon/LoteeYoon.github.io/blob/master/correlation.JPG?raw=true"></p>
### 2. Non-stationary targets











  Q-learning은 Model-free RL이므로 Agent는 환경과의 상호작용으로 발생하는 일련의 Observation(s), Action(a), Reward(r)를 통해 학습한다. Q-learning은 off-policy TD 알고리즘이고 이에 대한 Optimal action-value Q를 나타내면 다음과 같다.  

$$
\begin{aligned}
Q^*(s, a) = \mathbb{E}_{s' \sim \epsilon} [ r + \gamma max_{a'} Q^*(s', a') \mid s, \; a ]\\
\end{aligned}
$$

  이때 network를 function approximator로 Optimal action-value function Q를 근사할 수 있다.  

$$
\begin{aligned}
Q(s, a; \theta) \simeq Q^*(s, a)\\
\end{aligned}
$$

  Q-function을 parameter $\theta$ 로 근사한 network를 Q-network라고 한다. 그리고 Loss function을 다음과 같이 Squared Error 형태로 정의할 수 있다.

$$
\begin{aligned}
L_{i}(\theta_{i}) &= \mathbb{E}_{s, a \sim \rho(\cdot)} [ (y_i - Q(s, a; \theta_{i}))^2 ]\\
where, \; y_i &= \mathbb{E}_{s' \sim \epsilon} [ r + \gamma \max_{a'} Q(s', a'; \theta_{i-1}) \mid s, \; a]\\
\end{aligned}
$$


  하지만 RL에서는 Q-function을 나타내기 위해 Neural network같은 Non-linear function approximator를 사용하면 학습이 제대로 되지 않는 것으로 알려져 있다.




  이는 [Problem](#problem)에서 설명한 문제들 때문이다.

### 1. Experience replay

  RL에서의 학습 데이터는 시간의 흐름에 따라 sequential하게 수집되고, 이 sequential data는 인접한 것끼리 높은 correlation을 갖게 된다. 이러한 데이터를 그대로 사용하게 되면

### 2. Target Network


## Methods
