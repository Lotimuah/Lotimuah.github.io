---
layout: article
title: DQN 논문 정리
tags: Paper_Review
#comments: true
#article_header:
#  type: cover
#  image:
#    src:
aside:
  toc: true
key: page-aside
---

  \* 이 포스트는 DeepMind의 [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) 논문과 [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) 논문에 대한 내용을 정리한 글입니다.

  ----------------------------------------------------------------------

**분류** : Model-Free/Deep Q-Learning/DQN  


## Problem

  &nbsp;&nbsp;이 논문이 발표되기 전까지 vision이나 speech같은 high-dimensional sensory data로부터 agent를 학습시키는 것은 RL의 오랜 과제였습니다. 최근의 Deep Learning의 발전으로 고차원의 feature를 얻는 것이 가능해지면서 이를 RL에도 적용하려는 연구가 진행되었습니다. 그러나 Deep Learning을 RL에 적용하는 과정에서 몇 가지 문제를 발견하게 됩니다.  

  1. Deep Learning의 학습에는 labeling된 대량의 데이터가 필요한데, RL에서는 label이 없는 reward로 학습하며 심지어 sparse, noisy, delayed 하므로 학습에 어려움이 있습니다.  

  2. 대부분의 Deep Learning 문제에서는 data의 independent를 가정하지만, sequential decision 문제를 다루는 RL의 특성상 data간의 correlation이 존재하기 때문에 가정이 성립하지 않습니다. (Correlation Problem)  

  3. RL에서는 알고리즘이 새로운 behavior를 학습할 때마다 data의 distribution이 변하게 되므로 학습이 불안정합니다. (Non-Stationary Problem)  

  &nbsp;&nbsp;본 논문에서는 기존의 Q-learning에서 Q-table을 network로 대체하여 raw image data로부터 control policy를 학습한다. 또한 **Experience Replay Memory**와 **Target network**같은 아이디어들을 사용해 deep RL의 문제들을 해결했다.


<!-- - high-dimensional sensory input이 주어졌을 때, agent를 효과적으로 학습시키는 deep learning 알고리즘을 제안  
- CNN model을 사용해 raw pixel을 input으로 받으면 future reward를 추정하는 value function을 output  
- Atari 2600의 6종 아케이드 게임에서 기존 연구를 능가했으며, 그 중 3종의 게임에 대해서 human expert를 능가 -->


## Objective



## Methods
