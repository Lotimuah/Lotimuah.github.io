---
layout: article
title: PER 논문 정리
tags: Paper_Review
#comments: true
#article_header:
#  type: cover
#  image:
#    src:
aside:
  toc: true
key: page-aside
use_math: true
---

\* 이 포스트는 DeepMind의 [Prioritized Experience Replay](https://arxiv.org/pdf/1511.05952.pdf) 논문을 정리한 글입니다.

----------------------------------------------------------------------

**분류** : Model-free/Deep Q-learning/PER


## Motivation

  이전 포스팅 [DQN 논문 정리](https://loteeyoon.github.io/2022/01/04/DQN-%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC.html)에서 DQN에 사용된 **Experience Replay** 방법은 환경과의 상호작용으로 얻은 experience를 저장하고 이를 random하게 sampling하여 sample data들이 가지는 correlation을 없앴고 sample들을 중복 사용함으로써 효율적인 학습을 가능하게 했습니다. 그러나 우리는 현실의 모든 경험에는 중요도가 존재한다는 것을 알고 있습니다. 그래서 논문에서는 이러한 uniform random sampling 방식으로 모든 경험을 균등하게 취급할 것이 아니라 더 중요하고 희소한 경험에 더 높은 Priority를 매길 것을 제안합니다. 이것이 PER의 핵심 아이디어라고 볼 수 있습니다.

## Prioritized Replay

  Replay memory를 사용할 때는 두 가지 측면에서 선택을 해야 합니다. 하나는 어떤 experience를 저장할 것인지에 대한 것이고, 다른 하나는 어떤 experience를 replay할 것인가 입니다. 이 논문에서는 후자의 측면에서만 접근합니다.

### 1. Prioritizing with TD-error

  Prioritized replay에서 가장 중요한 요소는 각 transition의 중요도를 측정할 criterion입니다. 이상적인 criterion은 현재 state에서 agent가 transition으로부터 얼마나 배울 수 있는지에 대한 정량적인 값인데, 이 값은 직접적으로 구할 수 있는 것이 아니기 때문에 논문에서는 대신 TD-error $\delta$를 사용합니다. TD-error는 Q-learning에서 target과 prediction의 차이를 의미하고 다음과 같이 나타낼 수 있습니다.

$$
\begin{aligned}
\delta = R_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, A_t)
\end{aligned}
$$

  이 $\delta$는 agent 입장에서 해당 transition이 얼마나 surprising한지를 나타내는데, 구체적으로는 다음 step의 bootstrap estimate에 비해 value가 얼마나 차이가 있는지를 의미합니다. 이 값을 이용해 논문에서는 각 경험의 가중치를 **priority**로 다음과 같이 정의합니다.

$$
\begin{aligned}
p_t = |\delta| + \epsilon
\end{aligned}
$$

  TD-error의 절댓값에 아주 작은 constant 값으로 $\epsilon$을 더해주는데, 이는 priority가 0이 되어버리는 경우 해당 경험은 이후 뽑히지 않게 되므로 이를 방지하기 위함입니다.


### 2. Stochastic Prioritization

  Priority $p_t$를 기준으로만 greedy하게 sampling을 하게 되면 priority가 높은 몇 개의 sample만 학습에 사용되어 overfitting 가능성이 높아지기 때문에 다양한 transition을 학습하기 위해서 논문에서는 uniform sampling과 greedy sampling의 중간 수준인 stochastic sampling을 제안하고 있습니다. 이를 위해 hyperparameter $\alpha$를 통해 **sampling probability**를 다음과 같이 조절할 수 있습니다.

$$
\begin{aligned}
P(i) = \frac{p^{\alpha}_i}{\sum_{k} p^{\alpha}_i}\\
\end{aligned}
$$

<br/>
  $\alpha$는 [0, 1] 사이의 값으로, 0이 되면 uniform sampling과 동일해지고 1이 되면 greedy sampling이 됩니다. 따라서 $\alpha$를 통해 얼마나 priority에 의한 sampling을 허용할지를 정할 수 있습니다.


### 3. Annealing the bias

  일반적으로 Non-uniform sampling에는 bias가 발생합니다. PER의 경우 stochastic sampling에 의해 replay memory의 sample 분포와 sampling된 sample 분포간의 차이가 생기게 되고 이는 추정하려는 값에 변화를 일으킵니다. 논문에서는 **Importance-sampling(IS)** weight $w_i$를 도입하여 이러한 문제를 해결합니다. 즉, 기존의 TD-error $\delta$대신 weight을 적용한 $w_i \delta$를 사용해 bais를 보정해주는 것입니다.

$$
\begin{aligned}
w_i = (\frac{1}{N} \cdot \frac{1}{P(i)})^{\beta}
\end{aligned}
$$

  N은 replay memory의 크기를 의미하고, $\beta$는 [0, 1] 사이의 값이으로 bias에 대한 보정을 얼마나 할 것인지 결정합니다. $\beta$로 인해 sampling probability $P(i)$가 클수록 weight $w_i$가 작아지는 효과를 얻고, 따라서 $P(i)$가 클수록 학습에 사용되는 TD-error는 작아집니다. 이러한 효과는 $\beta$가 1에 가까울수록 커집니다. 일반적으로 강화학습에서 학습 후반에 사용된 sample 분포의 bais가 큰 영향을 미치기 때문에 논문에서는 학습이 진행됨에 따라 $\beta$를 선형적으로 증가시킵니다.
