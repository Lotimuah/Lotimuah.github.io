---
layout: article
title: PER 논문 정리
tags: Paper_Review
#comments: true
#article_header:
#  type: cover
#  image:
#    src:
aside:
  toc: true
key: page-aside
use_math: true
---

\* 이 포스트는 DeepMind의 [Prioritized Experience Replay](https://arxiv.org/pdf/1511.05952.pdf) 논문을 정리한 글입니다.

----------------------------------------------------------------------

**분류** : Model-free/Deep Q-learning/PER


## Motivation

  이전 포스팅 [DQN 논문 정리](https://loteeyoon.github.io/2022/01/04/DQN-%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC.html)에서 DQN에 사용된 **Experience Replay** 방법은 환경과의 상호작용으로 얻은 experience를 저장하고 이를 random하게 sampling하여 sample data들이 가지는 correlation을 없앴고 sample들을 중복 사용함으로써 효율적인 학습을 가능하게 했습니다. 그러나 우리는 현실의 모든 경험에는 중요도가 존재한다는 것을 알고 있습니다. 그래서 논문에서는 이러한 uniform random sampling 방식으로 모든 경험을 균등하게 취급할 것이 아니라 더 중요하고 희소한 경험에 더 높은 Priority를 매길 것을 제안합니다. 이것이 PER의 핵심 아이디어라고 볼 수 있습니다.
